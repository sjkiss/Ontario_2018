<<<<<<< HEAD
<<<<<<< HEAD
select(policy_polarization)
# primary media online
policy_online <- on18 %>%
filter(Primary_media == "Online") %>%
select(policy_polarization)
#primary media social media
policy_social_media <- on18 %>%
filter(Primary_media == "Social_Media") %>%
select(policy_polarization)
#primary media mixed
policy_mixed <- on18 %>%
filter(Primary_media == "Mixed") %>%
select(policy_polarization)
### Social Use
#high social use
policy_often <- on18 %>%
filter((Social_Use2 == "Several times a day" |
Social_Use2 == "About once a day" |
Social_Use2 == "Several times a week" |
Social_Use2 == "About once a week"  |
Social_Use2 == "Several times a month")) %>%
select(policy_polarization)
# low social use
policy_rarely <- on18 %>%
filter((Social_Use2 == "Several times in a year" |
Social_Use2 == "Never" | Social_Use2 == "About once a month")) %>%
select(policy_polarization)
bimodality_coefficient(policy_often, na.rm = T)
bimodality_coefficient(policy_rarely, na.rm = T)
#### Affective Polarization Wagner (2021) ####
#Extract feelings towards parties scores
on18 %>%
select(starts_with("partyeval")) %>%
glimpse()
# so it looks like I took the step sometime of
# scaling these variables to 0 and 1, and they end in
# _out.
on18 %>%
select(starts_with("partyeval")) %>%
summary()
#Looks like the originals run from 0 to 5
on18 %>%
select(starts_with("partyeval")) %>%
val_labels()
on18 %>%
select(starts_with("partyeval")) %>%
var_label()
#Party Numbers
#4 is Green
# 1 is Liberal
# 3 is NDP
# 2 is OPC
# 0 (0) is Really dislike and 5 (1) is really like.
#Extract _out and create a new dataset to calculate affective polarization scores
on18 %>%
select(id,starts_with("partyeval")&ends_with("out"))->affect
names(affect)
affect %>%
rename(Green_Like=2, Liberal_Like=3, NDP_Like=4, PC_Like=5)->affect
#Create a new dataset for the weighted score
affect_wt <- affect
#For each respondent, we will need an average like for the parties
# This is mean(like)_i
#So we need to effectively calculate a mean for each row.
#Each row is one R.
affect %>%
rowwise() %>%
mutate(mean_like=mean(c_across(2:5), na.rm=T))->affect
#Now we need to take each R's like score for each party
# And subtract the mean from it.
affect_pol_cal <- affect %>%
pivot_longer(cols = !c(id, mean_like),
names_to = "Party",
values_to = "Party_like_score")
affect_pol_cal
affect_pol_cal$like_mean <- (affect_pol_cal$Party_like_score - affect_pol_cal$mean_like)^2
affect_pol_cal %>%
group_by(id) %>%
summarise(Soc_dis = sum(like_mean)) -> Soc_dis_scores
Soc_dis_scores$Soc_dis <- sqrt(Soc_dis_scores$Soc_dis/4)
full_join(on18, Soc_dis_scores, by = join_by(id)) -> on18
#### Weighted Affective Polarization (WAP) Scores ####
affect_wt$Green_Like_wt <- (affect$Green_Like * 0.046)
affect_wt$PC_Like_wt <- (affect$PC_Like * 0.405)
affect_wt$Liberal_Like_wt <- (affect$Liberal_Like * 0.196)
affect_wt$NDP_Like_wt <- (affect$NDP_Like * 0.336)
affect_wt %>%
rowwise() %>%
mutate(mean_like=mean(c_across(6:9), na.rm=T))->affect_wt
affect_pol_cal_wt <- affect_wt %>%
select(., !(6:9)) %>%
pivot_longer(cols = !c(id, mean_like),
names_to = "Party",
values_to = "Party_like_score")
affect_pol_cal_wt %>%
mutate(vote_share=case_when(Party == "Green_Like" ~ 0.046, Party == "Liberal_Like" ~ 0.196, Party == "NDP_Like" ~ 0.336,
Party == "PC_Like" ~ 0.405
)) -> affect_pol_cal_wt
affect_pol_cal_wt$like_mean <- (affect_pol_cal_wt$Party_like_score - affect_pol_cal_wt$mean_like)^2
affect_pol_cal_wt$like_mean_wt <- (affect_pol_cal_wt$like_mean * affect_pol_cal_wt$vote_share)
affect_pol_cal_wt %>%
group_by(id) %>%
summarise(WAP = sum(like_mean_wt)) -> WAP_scores
WAP_scores$WAP <- sqrt(WAP_scores$WAP)
full_join(on18, WAP_scores, by = join_by(id)) -> on18
on18 <- on18 %>%
mutate(
WAP_sd = as.numeric(scale(WAP))
)
on18 %>%
group_by(Primary_media) %>%
summarise(mean = mean(pointerst_ONint, na.rm = T), sd = sd(pointerst_ONint, na.rm = T))
on18 %>%
group_by(Social_Use2) %>%
summarise(mean = mean(WAP, na.rm = T), sd = sd(WAP, na.rm = T))
#### WAP Leaders ####
#leader 1 Ford, leader 2 Horwath, leader 3 Schreiner, leader 4 Wynne
leader_affect <- on18 %>%
select(c(id, leadereval_1_out:leadereval_4_out)) %>%
rename(
Ford = leadereval_1_out,
Horwath = leadereval_2_out,
Schreiner = leadereval_3_out,
Wynne = leadereval_4_out
)
leader_affect$Schreiner_Like_wt <- (leader_affect$Schreiner * 0.046)
leader_affect$Ford_Like_wt <- (leader_affect$Ford * 0.405)
leader_affect$Wynne_Like_wt <- (leader_affect$Wynne * 0.196)
leader_affect$Horwath_Like_wt <- (leader_affect$Horwath * 0.336)
leader_affect %>%
rowwise() %>%
mutate(mean_like=mean(c_across(6:9), na.rm=T))->leader_affect
leader_affect <- leader_affect %>%
select(., !(6:9)) %>%
pivot_longer(cols = !c(id, mean_like),
names_to = "Leader",
values_to = "Leader_like_score")
leader_affect %>%
mutate(vote_share=case_when(Leader == "Schreiner" ~ 0.046, Leader == "Wynne" ~ 0.196, Leader == "Horwath" ~ 0.336,
Leader == "Ford" ~ 0.405
)) -> leader_affect
leader_affect$like_mean <- (leader_affect$Leader_like_score - leader_affect$mean_like)^2
leader_affect$like_mean_wt <- (leader_affect$like_mean * leader_affect$vote_share)
leader_affect %>%
group_by(id) %>%
summarise(WAP_lead = sum(like_mean_wt)) -> leader_affect
leader_affect$WAP_lead <- sqrt(leader_affect$WAP_lead)
full_join(on18, leader_affect, by = join_by(id)) -> on18
on18$wap_difference <- on18$WAP - on18$WAP_lead
on18 <- on18 %>%
mutate(
WAP_lead_sd = as.numeric(scale(WAP_lead))
)
mean(on18$wap_difference, na.rm = T)
#### DESCRIPTIVE STATISTICS ####
#### Primary Media ####
#Descriptive Statistics for political knowledge by Primary media
table(on18$Social_Use2, on18$pol_knowledge)
on18 %>%
group_by(Primary_media) %>%
summarise(mean = mean(pol_knowledge, na.rm = T), sd = sd(pol_knowledge, na.rm = T))
#### Primary Media and Media source ####
on18$HuffingtonPostonline
media_Sources_by_primarymedia <- on18 %>%
group_by(Primary_media) %>%
summarise(across(CBCTV:HuffingtonPostonline, \(x)mean(x))) %>%
filter(Primary_media == "Online") %>%
pivot_longer(cols = 2:22)
#see the most read sources for online news users
media_Sources_by_primarymedia %>%
ggplot(aes(y = name, x = value)) + geom_point()
#### Affective Polarization ####
on18 %>%
group_by(Social_Use2) %>%
summarise(mean = mean(WAP, na.rm = T), sd = sd(WAP, na.rm = T), n=n(), se=sd/sqrt(n))->mean_affect_scores
(on18 %>%
group_by(partyvote2018) %>%
summarise(Mean = mean(WAP, na.rm = T), Standard_Deviation = sd(WAP, na.rm = T), Minimum = min(WAP, na.rm = T), Maximum = max(WAP, na.rm = T), N=n()) %>%
rename(Group = partyvote2018) -> affect_scores_party)
(on18 %>%
group_by(Primary_media) %>%
summarise(Mean = mean(WAP, na.rm = T), Standard_Deviation = sd(WAP, na.rm = T), Minimum = min(WAP, na.rm = T), Maximum = max(WAP, na.rm = T), N=n()) %>%
rename(Group = Primary_media) -> affect_scores_primary_media)
mean(on18$WAP, na.rm = T); sd(on18$WAP, na.rm = T)
descripts_WAP <- rbind(tibble(Group = "Ontarians", Mean = mean(on18$WAP, na.rm = T),
Standard_Deviation = sd(on18$WAP, na.rm = T),
Minimum = min(on18$WAP, na.rm = T),
Maximum = max(on18$WAP, na.rm = T),
N = (length(!is.na(on18$WAP)))),
on18 %>%
group_by(Social_Use2) %>%
summarise(Mean = mean(WAP, na.rm = T), Standard_Deviation = sd(WAP, na.rm = T), Minimum = min(WAP, na.rm = T), Maximum = max(WAP, na.rm = T), N=n()) %>%
rename(Group = Social_Use2), affect_scores_party, affect_scores_primary_media)
kableExtra::kable(descripts_WAP, format = "pipe", digits = 3)
tibble(Group = "Ontarians", Mean = mean(on18$WAP, na.rm = T),
Standard_Deviation = sd(on18$WAP, na.rm = T),
Minimum = min(on18$WAP, na.rm = T),
Maximum = max(on18$WAP, na.rm = T),
N = (length(!is.na(on18$WAP))))
mean_affect_scores
mean_affect_scores %>%
filter(complete.cases(.)) %>%
ggplot(., aes(x = mean,
y = Social_Use2,
colour = Social_Use2)) +
geom_point()+
geom_errorbar(aes (xmin = mean - (1.96*se), xmax = mean + (1.96*se)), width =.2) +
labs(x = "Mean", y= "Social Media Use") +
theme_bw()  +
theme(axis.text.y=element_blank(),
axis.ticks.y = element_blank(),
strip.text = element_text(size=8))+
guides(color=guide_legend(reverse=T))+xlim(c(0,0.5)) -> affect_scores_social_use; affect_scores_social_use
on18 %>%
group_by(Primary_media) %>%
summarise(mean = mean(Soc_dis, na.rm = T), sd = sd(Soc_dis, na.rm = T))
#### Policy Position Distribution ####
#Exploratory factor analysis
cor_policies <- on18 %>%
dplyr::select(c(help_racial_minorities, help_women, more_coporate_tax,
more_personal_tax, income_inequality, drug_benefit_u25, free_post_secondary,
lr_private_health_care, Lr_minimum_wage_to_high_prices, lr_business_benefits_everyone,
lr_inappropriate_sex_ed
)) %>%
cor(., use = "complete.obs")
eigen(cor_policies)$values
(policy_fa_analysis <- psych::fa(cor_policies, nfactors = 3))
policy_issues <- 'policy =~ help_racial_minorities + help_women + more_coporate_tax +
more_personal_tax + income_inequality + drug_benefit_u25 + free_post_secondary +
lr_private_health_care + Lr_minimum_wage_to_high_prices + lr_business_benefits_everyone +
lr_inappropriate_sex_ed'
cfa_policies <- cfa(policy_issues, data = on18)
summary(cfa_policies, fit.measures = T, standardized = T)
#### REGRESION MODELS ####
#Create regression models for affective polarization using the weighted affective polarization (WAP) measure
#**** Start thinking about control variables
WAP_reg <- list()
WAP_graph <- list()
WAP_reg[[1]] <- lm(WAP ~ Primary_media, data = on18, na.action = na.omit);summary(WAP_reg[[1]])
WAP_graph[[1]] <- graph_regression(WAP_reg[[1]]); WAP_graph[[1]]
WAP_reg[[2]] <- lm(WAP_sd ~ Interest, data = on18, na.action = na.omit); summary(WAP_reg[[2]])
WAP_graph[[2]] <- graph_regression(WAP_reg[[2]]); WAP_graph[[2]]
WAP_reg[[3]] <- lm(WAP_sd ~ Interest + Primary_media, data = on18, na.action = na.omit); summary(WAP_reg[[3]])
WAP_graph[[3]] <- graph_regression(WAP_reg[[3]]); WAP_graph[[3]]
WAP_reg[[4]] <- lm(WAP_sd ~ Interest + Primary_media + age3, data = on18, na.action = na.omit); summary(WAP_reg[[4]])
WAP_graph[[4]] <- graph_regression(WAP_reg[[4]]); WAP_graph[[4]]
WAP_reg[[5]] <- lm(WAP_sd ~ Interest + Primary_media + age3 + degree, data = on18, na.action = na.omit); summary(WAP_reg[[5]])
WAP_graph[[5]] <- graph_regression(WAP_reg[[5]]); WAP_graph[[5]]
WAP_reg[[6]] <- lm(WAP_sd ~ Interest + Primary_media + age3 + degree + income3, data = on18, na.action = na.omit); summary(WAP_reg[[6]])
WAP_graph[[6]] <- graph_regression(WAP_reg[[6]]); WAP_graph[[6]]
WAP_reg[[7]] <- lm(WAP_sd ~ Interest + Primary_media + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit); summary(WAP_reg[[7]])
WAP_graph[[7]] <- graph_regression(WAP_reg[[7]]); WAP_graph[[7]]
WAP_Interact <- lm(WAP_sd ~ Primary_media*Interest, data = on18, na.action = na.omit)
summary(WAP_Interact)
WAP_Interact2 <- lm(WAP_sd ~ Primary_media*Interest  + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit)
summary(WAP_Interact2)
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
marginaleffects::plot_slopes(WAP_Interact2, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
#Display Models
modelsummary(WAP_reg, stars = T, vcov = "HC0") #As numbers
WAP_reg_df <- map(WAP_reg, tidy)
WAP_reg_df_ci <- map(WAP_reg, confint, level = 0.95, HC_type = "HC0")
WAP_reg_df_ci <- map(WAP_reg_df_ci, as_tibble)
WAP_reg_df <- bind_rows(WAP_reg_df, .id = "model_num")
WAP_reg_df_ci <- bind_rows(WAP_reg_df_ci) %>%
rename("conf.low" = "2.5 %", "conf.high" = "97.5 %")
WAP_reg_df <- bind_cols(WAP_reg_df, WAP_reg_df_ci)
model_names <- list(
"1" = "Model 1",
"2" = "Model 2",
"3" = "Model 3",
"4" = "Model 4",
"5" = "Model 5",
"6" = "Model 6",
"7" = "Model 7"
)
model_labeller <- function(variable, value){
return(model_names[value])
}
WAP_reg_df %>%
filter(!term == "(Intercept)") %>%
ggplot(aes(x = estimate, y = term, xmax = conf.high, xmin = conf.low)) + geom_point() +
geom_linerange() + geom_vline(xintercept = 0, lty = 4) +
facet_wrap(~model_num, ncol = 4, labeller = model_labeller) + theme_bw()
#Investigating the age coefficient
summary(lm(WAP_sd ~ age, data = on18))
summary(lm(WAP_sd ~ age + I(age^2), data = on18))
age_poly <- lm(WAP_sd ~ Interest + Primary_media  + age + I(age^2), data = on18, na.action = na.omit); summary(age_poly)
summary(lm(WAP_sd ~ Interest + Primary_media + age + I(age^2) + degree + income3 + pol_knowledge, data = on18, na.action = na.omit))
age_legacy <- data.frame(age = seq(18, 100, 1),
Primary_media = "Legacy",
Interest = mean(on18$Interest, na.rm = T))
age_legacy[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_legacy, interval = "confidence")
age_legacy$media <- "Legacy"
age_mixed <- data.frame(age = seq(18, 100, 1),
Primary_media = "Mixed",
Interest = mean(on18$Interest, na.rm = T))
age_mixed[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_mixed, interval = "confidence")
age_mixed$media <- "Mixed"
age_online <- data.frame(age = seq(18, 100, 1),
Primary_media = "Online",
Interest = mean(on18$Interest, na.rm = T))
age_online[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_online, interval = "confidence")
age_online$media <- "Online"
age_social_media <- data.frame(age = seq(18, 100, 1),
Primary_media = "Social_Media",
Interest = mean(on18$Interest, na.rm = T))
age_social_media[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_social_media, interval = "confidence")
age_social_media$media <- "Social Media"
age_predicted <- rbind(age_legacy, age_mixed, age_online, age_social_media)
age_predicted_graph <- age_predicted %>%
ggplot(aes(x = age, y = predicted_WAP, ymin = conf_low, ymax = conf_up)) +
geom_line(colour = "#008080") +  facet_wrap(~media) +
geom_ribbon(alpha = 0.2, fill = "aquamarine3") +
labs(x = "Age", y = "Predicted Level of Affective Polarization \n (WAP Score)") +
theme_bw()
#### Replicate models with the social use variable ####
WAP_reg_soc_use <- list()
WAP_reg_soc_use[[1]] <- lm(WAP_sd ~ Social_Use2, data = on18, na.action = na.omit);summary(WAP_reg_soc_use[[1]])
#WAP_graph[[1]] <- graph_regression(WAP_reg_soc_use[[1]]); WAP_graph[[1]]
WAP_reg_soc_use[[2]] <- lm(WAP_sd ~ Interest, data = on18, na.action = na.omit); summary(WAP_reg_soc_use[[2]])
#WAP_graph[[2]] <- graph_regression(WAP_reg_soc_use[[2]]); WAP_graph[[2]]
WAP_reg_soc_use[[3]] <- lm(WAP_sd ~ Interest + Social_Use2, data = on18, na.action = na.omit); summary(WAP_reg_soc_use[[3]])
#WAP_graph[[3]] <- graph_regression(WAP_reg_soc_use[[3]]); WAP_graph[[3]]
WAP_reg_soc_use[[4]] <- lm(WAP_sd ~ Interest + Social_Use2 + age3, data = on18, na.action = na.omit); summary(WAP_reg_soc_use[[4]])
#WAP_graph[[4]] <- graph_regression(WAP_reg_soc_use[[4]]); WAP_graph[[4]]
WAP_reg_soc_use[[5]] <- lm(WAP_sd ~ Interest + Social_Use2 + age3 + degree, data = on18, na.action = na.omit); summary(WAP_reg_soc_use[[5]])
#WAP_graph[[5]] <- graph_regression(WAP_reg_soc_use[[5]]); WAP_graph[[5]]
WAP_reg_soc_use[[6]] <- lm(WAP_sd ~ Interest + Social_Use2 + age3 + degree + income3, data = on18, na.action = na.omit); summary(WAP_reg_soc_use[[6]])
#WAP_graph[[6]] <- graph_regression(WAP_reg_soc_use[[6]]); WAP_graph[[6]]
WAP_reg_soc_use[[7]] <- lm(WAP_sd ~ Interest + Social_Use2 + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit); summary(WAP_reg_soc_use[[7]])
#WAP_graph[[7]] <- graph_regression(WAP_reg_soc_use[[7]]); WAP_graph[[7]]
modelsummary(WAP_reg_soc_use, stars = T, vcov = "HC0")
#ggarrange(plotlist = list(WAP_primarymedia_graph, WAP_Interest_Graph, WAP_Interact_graph))
#### Affective polarization models ####
#Create regression models for affective polarization using the weighted affective polarization (WAP) measure
WAP_lead_reg <- list()
WAP_lead_graph <- list()
WAP_lead_reg[[1]] <- lm(WAP_lead_sd ~ Primary_media, data = on18, na.action = na.omit);summary(WAP_lead_reg[[1]])
WAP_lead_graph[[1]] <- graph_regression(WAP_lead_reg[[1]]); WAP_lead_graph[[1]]
WAP_lead_reg[[2]] <- lm(WAP_lead_sd ~ Interest, data = on18, na.action = na.omit); summary(WAP_lead_reg[[2]])
WAP_lead_graph[[2]] <- graph_regression(WAP_lead_reg[[2]]); WAP_lead_graph[[2]]
WAP_lead_reg[[3]] <- lm(WAP_lead_sd ~ Interest + Primary_media, data = on18, na.action = na.omit); summary(WAP_lead_reg[[3]])
WAP_lead_graph[[3]] <- graph_regression(WAP_lead_reg[[3]]); WAP_lead_graph[[3]]
WAP_lead_reg[[4]] <- lm(WAP_lead_sd ~ Interest + Primary_media + age3, data = on18, na.action = na.omit); summary(WAP_lead_reg[[4]])
WAP_lead_graph[[4]] <- graph_regression(WAP_lead_reg[[4]]); WAP_lead_graph[[4]]
WAP_lead_reg[[5]] <- lm(WAP_lead_sd ~ Interest + Primary_media + age3 + degree, data = on18, na.action = na.omit); summary(WAP_lead_reg[[5]])
WAP_lead_graph[[5]] <- graph_regression(WAP_lead_reg[[5]]); WAP_lead_graph[[5]]
WAP_lead_reg[[6]] <- lm(WAP_lead_sd ~ Interest + Primary_media + age3 + degree + income3, data = on18, na.action = na.omit); summary(WAP_lead_reg[[6]])
WAP_lead_graph[[6]] <- graph_regression(WAP_lead_reg[[6]]); WAP_lead_graph[[6]]
WAP_lead_reg[[7]] <- lm(WAP_lead_sd ~ Interest + Primary_media + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit); summary(WAP_lead_reg[[7]])
WAP_lead_graph[[7]] <- graph_regression(WAP_lead_reg[[7]]); WAP_lead_graph[[7]]
WAP_lead_Interact <- lm(WAP_lead_sd ~ Primary_media*Interest, data = on18, na.action = na.omit)
summary(WAP_lead_Interact)
WAP_lead_Interact2 <- lm(WAP_lead_sd ~ Primary_media*Interest + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit)
summary(WAP_lead_Interact2)
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_lead_Interact, variables = "Interest", condition = "Primary_media") +
geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + coord_flip() + theme_bw()
marginaleffects::plot_slopes(WAP_lead_Interact2, variables = "Interest", condition = "Primary_media") +
geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + coord_flip() + theme_bw()
#Display Models
modelsummary(WAP_lead_reg, stars = T, vcov = "HC0") #As numbers
#### BIMODALITY COEFFICENT ####
#### Primary Media ####
bimod_online <- bimodality_coefficient(policy_online, na.rm = T); bimod_online
bimod_leacgy <- bimodality_coefficient(policy_legacy, na.rm = T); bimod_leacgy
bimodality_coefficient(policy_social_media, na.rm = T)
bimodality_coefficient(policy_mixed, na.rm = T)
bimod_legacy <- policy_legacy %>%
ggplot() + geom_density(aes(policy_polarization), col = "seagreen", fill = "seagreen", alpha = 0.4) +
labs(title = "Legacy Media", subtitle = "Bimodality Coefficent (0.29)", x = "Policy Positions", y = NULL) +
theme_bw()
bimod_online <- policy_online %>%
ggplot() + geom_density(aes(policy_polarization), col = "seagreen", fill = "seagreen", alpha = 0.4) +
labs(title = "Online Media", subtitle = "Bimodality Coefficent (0.36)", x = "Policy Positions", y = NULL) +
theme_bw()
bimod_social <- policy_social_media %>%
ggplot() + geom_density(aes(policy_polarization), col = "seagreen", fill = "seagreen", alpha = 0.4) +
labs(title = "Social Media", subtitle = "Bimodality Coefficent (0.31)", x = "Policy Positions", y = NULL) +
theme_bw()
bimod_mixed <- policy_mixed %>%
ggplot() + geom_density(aes(policy_polarization), col = "seagreen", fill = "seagreen", alpha = 0.4) +
labs(title = "Mixed media", subtitle = "Bimodality Coefficent (0.32)", x = "Policy Positions", y = NULL) +
theme_bw()
gridExtra::grid.arrange(bimod_legacy, bimod_online, bimod_social, bimod_mixed)
summary(lm(imm_sentiment ~ WAP, data = on18))
summary(lm(fin_sentiment ~ WAP, data = on18))
summary(lm(imm_sentiment ~ WAP, data = on18))
summary(lm(fin_sentiment ~ WAP, data = on18))
summary(lm(imm_sentiment ~ WAP, data = on18))
Interest_pm
Interest_pm <- lm(Interest ~ Primary_media, data = on18, na.action = na.omit); summary(Interest_pm)
Interest_su <- lm(Interest ~ Social_Use2, data = on18, na.action = na.omit); summary(Interest_su)
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
source("~/Documents/Stats/Ontario_2018/Code/3_polarization.R", echo=TRUE)
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
marginaleffects::plot_slopes(WAP_Interact2, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
WAP_Interact2 <- lm(WAP_sd ~ Social_Use2*Interest  + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit)
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
marginaleffects::plot_slopes(WAP_Interact2, variables = "Interest", condition = "Primary_media") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
WAP_Interact2 <- lm(WAP_sd ~ Social_Use2*Interest  + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit)
marginaleffects::plot_slopes(WAP_Interact2, variables = "Interest", condition = "Social_Use2") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Primary_media", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
marginaleffects::plot_slopes(WAP_Interact2, variables = "Primary_media", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
marginaleffects::plot_slopes(WAP_Interact2, variables = "Social_Use", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
marginaleffects::plot_slopes(WAP_Interact2, variables = "Social_Use2", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
WAP_Interact2 <- lm(WAP_sd ~ as.numeric(Social_Use2)*Interest  + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit)
summary(WAP_Interact2)
marginaleffects::plot_slopes(WAP_Interact2, variables = "Social_Use2", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
WAP_Interact2 <- lm(WAP_sd ~ as.numeric(Social_Use2)*Interest  + age3 + degree + income3 + pol_knowledge, data = on18, na.action = na.omit)
summary(WAP_Interact2)
marginaleffects::plot_slopes(WAP_Interact2, variables = "Social_Use2", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + theme_bw()
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Primary_media", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + labs(facet = c("PM1", "PM2", "PM3")) + theme_bw()
#Visualize the marginal effects from the interaction effects
marginaleffects::plot_slopes(WAP_Interact, variables = "Primary_media", condition = "Interest") + geom_hline(yintercept  = 0, lty = "dashed", col = "forestgreen") + labs(y = "Marginal Effect of Primary Media Variable") + theme_bw()
?bam
??bam
mgcv::gam(WAP_sd ~ s(age3) Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
mgcv::gam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
mgcv::gam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18) %>%
modelsummary()
mgcv::bam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18) %>%
modelsummary()
mgcv::bam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
library(mgcv)
mgcv::gam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18) %>%
modelsummary()
mgcv::gam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
gam_model <- mgcv::gam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18) %>%
modelsummary()
gam_model <- mgcv::gam(WAP_sd ~ s(age3) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
modelsummary(Primary_media)
modelsummary(gam_model)
modelsummary(gam_model, stars = T)
draw(gam_model)
??draw
library(gratia)
draw(gam_model, residuals = T)
unique(on18$age3)
unique(on18$age2)
unique(on18$age1)
unique(on18$age)
on18 <- on18 %>%
mutate(age = replace(age, age == 117, NA))
on18$age2<-recode(on18$age, "45:100=1; 0:44=0; else=NA")
on18$age3<-scales::rescale(on18$age, to=c(0,1))
gam_model <- mgcv::gam(WAP_sd ~ s(age) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
draw(gam_model, residuals = T)
#Investigating the age coefficient
summary(lm(WAP_sd ~ age, data = on18))
summary(lm(WAP_sd ~ age + I(age^2), data = on18))
age_poly <- lm(WAP_sd ~ Interest + Primary_media  + age + I(age^2), data = on18, na.action = na.omit); summary(age_poly)
summary(lm(WAP_sd ~ Interest + Primary_media + age + I(age^2) + degree + income3 + pol_knowledge, data = on18, na.action = na.omit))
age_legacy <- data.frame(age = seq(18, 100, 1),
Primary_media = "Legacy",
Interest = mean(on18$Interest, na.rm = T))
age_legacy[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_legacy, interval = "confidence")
age_legacy$media <- "Legacy"
age_mixed <- data.frame(age = seq(18, 100, 1),
Primary_media = "Mixed",
Interest = mean(on18$Interest, na.rm = T))
age_mixed[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_mixed, interval = "confidence")
age_mixed$media <- "Mixed"
age_online <- data.frame(age = seq(18, 100, 1),
Primary_media = "Online",
Interest = mean(on18$Interest, na.rm = T))
age_online[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_online, interval = "confidence")
age_online$media <- "Online"
age_social_media <- data.frame(age = seq(18, 100, 1),
Primary_media = "Social_Media",
Interest = mean(on18$Interest, na.rm = T))
age_social_media[, c("predicted_WAP",
"conf_up",
"conf_low")] <- predict(age_poly, age_social_media, interval = "confidence")
age_social_media$media <- "Social Media"
age_predicted <- rbind(age_legacy, age_mixed, age_online, age_social_media)
age_predicted_graph <- age_predicted %>%
ggplot(aes(x = age, y = predicted_WAP, ymin = conf_low, ymax = conf_up)) +
geom_line(colour = "#008080") +  facet_wrap(~media) +
geom_ribbon(alpha = 0.2, fill = "aquamarine3") +
labs(x = "Age", y = "Predicted Level of Affective Polarization \n (WAP Score)") +
theme_bw()
age_predicted_graph
gam_model <- mgcv::gam(WAP_sd ~ s(age) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
draw(gam_model, residuals = T)
gam_model <- mgcv::gam(WAP ~ s(age) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
draw(gam_model, residuals = T)
gam_model <- mgcv::gam(WAP_sd ~ s(age) + Primary_media + Interest + degree + income3 + pol_knowledge, data = on18)
draw(gam_model, residuals = T)
=======
#### LSD ####
library(quanteda)
on18$fin.y
#source(here("Code/1_load_on18.R"))
#Now read in the spellchecked emotions file.
library(rio)
library(openxlsx)
out<-read.csv(here("Data/feelings_workfile_spellcheck.csv"))
#out<-import(here("Data/feelings_workfile_spellcheck_Aug27.xlsx"))
head(out)
out$indivfinfeel[14]
out$indivfinfeel[38]
on18$indivfinfeel %>%
map(., nchar) %>%
unlist() %>%
summary()
nrow(out)
nrow(on18)
mean(out$id)
mean(on18$id)
summary(on18$id)
summary(out$id)
length(on18$id)
length(out$id)
#Adjust the id#s
out$id<-out$id+37
head(out$id)
nrow(out)
nrow(on18)
#there is one extra row in on18 that did not get spellchecked. It doesn't look like a quality response, but it should be kept.
names(on18)
names(out)
summary(on18$id)
summary(out$id)
on18$immfeel
on18$indivfinfeel
#This file adds in spell-checked and piloted emotions files
on18$indivfinfeel[14]
on18$indivfinfeel[38]
#Note after this join indivfinfeel.x and immfeel.x ARE NOT SPELLCHECKED
#indivfinfeel.y and immfeel.y ARE SPELLCHECKED
on18 %>%
left_join(., out, by='id') -> on18
on18 %>%
rename(imm.x=immfeel.x, imm.y=immfeel.y, fin.x=indivfinfeel.x, fin.y=indivfinfeel.y)->on18
source(here("Code/1_load_on18.R"))
#Now read in the spellchecked emotions file.
library(rio)
library(openxlsx)
out<-read.csv(here("Data/feelings_workfile_spellcheck.csv"))
#out<-import(here("Data/feelings_workfile_spellcheck_Aug27.xlsx"))
head(out)
out$indivfinfeel[14]
out$indivfinfeel[38]
on18$indivfinfeel %>%
map(., nchar) %>%
unlist() %>%
summary()
nrow(out)
nrow(on18)
mean(out$id)
mean(on18$id)
summary(on18$id)
summary(out$id)
length(on18$id)
length(out$id)
#Adjust the id#s
out$id<-out$id+37
head(out$id)
nrow(out)
nrow(on18)
#there is one extra row in on18 that did not get spellchecked. It doesn't look like a quality response, but it should be kept.
names(on18)
names(out)
summary(on18$id)
summary(out$id)
on18$immfeel
on18$indivfinfeel
#This file adds in spell-checked and piloted emotions files
on18$indivfinfeel[14]
on18$indivfinfeel[38]
#Note after this join indivfinfeel.x and immfeel.x ARE NOT SPELLCHECKED
#indivfinfeel.y and immfeel.y ARE SPELLCHECKED
on18 %>%
left_join(., out, by='id') -> on18
on18 %>%
rename(imm.x=immfeel.x, imm.y=immfeel.y, fin.x=indivfinfeel.x, fin.y=indivfinfeel.y)->on18
on18$fin.y
source("Code/1_LSDprep_dec2017.R")
LSDprep_contr(on18$fin.y)
on18$fin.y.LSD<-LSDprep_contr(on18$fin.y)
on18$imm.y.LSD<-LSDprep_contr(on18$imm.y)
on18$imm.y.LSD
=======
>>>>>>> Simon_writing
on18$fin.y.LSD<-LSDprep_dict_punct(on18$fin.y.LSD)
on18$fin.y.LSD<-LSDprep_dict_punct(on18$fin.y.LSD)
on18$imm.y.LSD<-LSDprep_dict_punct(on18$imm.y.LSD)
on18$fin.y.LSD
on18$fin.y.LSD<-LSDprep_contr(on18$fin.y)
on18$imm.y.LSD<-LSDprep_contr(on18$imm.y)
return(LSDprep_dict_punct(on18$fin.y.LSD))
View(LSDprep_dict_punct(on18$fin.y.LSD))
on18$fin.y.LSD1<-LSDprep_dict_punct(on18$fin.y.LSD)
on18$imm.y.LSD1<-LSDprep_dict_punct(on18$imm.y.LSD)
on18 %>%
select(contains("imm.y.LSD"))
str_replace_all(o18$fin.y.LSD, pattern = "^", replacement = " ")
str_replace_all(on18$fin.y.LSD, pattern = "^", replacement = " ")
test<-LSDprep_dict_punct(on18$fin.y.LSD)
test
on18$fin.y.LSD<-LSDprep_contr(on18$fin.y)
on18$imm.y.LSD<-LSDprep_contr(on18$imm.y)
on18$imm.y.LSD1<-LSDprep_dict_punct(on18$imm.y.LSD)
on18$imm.y.LSD1
on18 %>%
select(imm.y) %>%
slice(285)
on18 %>%
ungroup() %>%
select(imm.y) %>%
slice(285)
on18 %>%
ungroup() %>%
select(imm.y) %>%
slice(285) %>%
View()
on18$fin.y.LSD<-LSDprep_contr(on18$fin.y)
on18$imm.y.LSD<-LSDprep_contr(on18$imm.y)
on18$fin.y.LSD1<-LSDprep_dict_punct(on18$fin.y.LSD)
on18$imm.y.LSD1<-LSDprep_dict_punct(on18$imm.y.LSD)
#Check that this worked
on18 %>%
select(contains("imm.y.LSD")) %>%
slice(196:198)
on18 %>%
ungroup() ->on18
on18 %>%   select(imm.y) %>%
slice(285) %>%
View()
#Check that this worked
on18 %>%
select(contains("imm.y.LSD")) %>%
slice(196:198)
#Check that this worked
on18 %>%
select(contains("imm.y.LSD1")) %>%
slice(197)
#Check that this worked
on18 %>%
filter(str_detect(ends_with("LSD1"), "xtoo"))
#Check that this worked
on18 %>%
filter(str_detect(imm.y.LSD1, "xtoo"))
#Check that this worked
on18 %>%
filter(str_detect(imm.y.LSD1, "xtoo")) %>%
select(contains(ends_with("LSD1")))
#Check that this worked
on18 %>%
filter(str_detect(imm.y.LSD1, "xtoo")) %>%
select(imm.y.LSD1)
LSDprep_punctspace(on18$fin.y.LSD1)
on18$fin.y.LSD2<-LSDprep_punctspace(on18$fin.y.LSD1)
on18$imm.y.LSD2<-LSDprep_punctspace(on18$imm.y.LSD1)
on18 %>%
select(ends_with("LSD2"))
on18 %>%
select(ends_with("LSD2")) %>%
print(100)
on18 %>%
select(ends_with("LSD2")) %>%
print(n=100)
LSDprep_negation(on18$fin.y.LSD2)
on18$fin.y.LSD2<-LSDprep_negation(on18$fin.y.LSD2)
on18$fin.y.LSD2<-LSDprep_punctspace(on18$fin.y.LSD1)
on18$imm.y.LSD2<-LSDprep_punctspace(on18$imm.y.LSD1)
on18$fin.y.LSD3<-LSDprep_negation(on18$fin.y.LSD2)
on18$fin.y.LSD3<-LSDprep_negation(on18$fin.y.LSD2)
on18$imm.y.LSD3<-LSDprep_negation(on18$imm.y.LSD2)
on18 %>%
select(ends_with("LSD3")) %>%
print(n=100)
on18 %>%
filter(str_detect(fin.y, pattern = " (N|n)ot much " ))
on18 %>%
filter(str_detect(fin.y, pattern = " (N|n)ot much " )) %>%
select(fin.y, fin.y.LSD3)
on18 %>%
filter(str_detect(fin.y, pattern = " (N|n)ot much " )) %>%
select(fin.y, fin.y.LSD3) %>% View()
#
LSDprep_dict(on18$fin.y.LSD3)
#
on18$fin.y.LSD4<-LSDprep_dict(on18$fin.y.LSD3)
on18$imm.y.LSD4<-LSDprep_dict(on18$imm.y.LSD3)
on18 %>%
select(ends_with("LSD4")) %>%
View()
#
# on18$fin.y.LSD4<-LSDprep_dict(on18$fin.y.LSD3)
# on18$imm.y.LSD4<-LSDprep_dict(on18$imm.y.LSD3)
# on18 %>%
#   select(ends_with("LSD4")) %>%
#   View()
#
# on18$fin.y.LSD4<-LSDprep_dict(on18$fin.y.LSD3)
# on18$imm.y.LSD4<-LSDprep_dict(on18$imm.y.LSD3)
# on18 %>%
#   select(ends_with("LSD4")) %>%
#   View()
library(quanteda)
?tokens_lookup
#
on18$fin.y.LSD4<-LSDprep_dict(on18$fin.y.LSD3)
on18$imm.y.LSD4<-LSDprep_dict(on18$imm.y.LSD3)
tokens(on18$fin.y.LSD4)
tokens_lookup(tokens(on18$fin.y.LSD4),
dictionary=data_dictionary_LSD2015)
data_dictionary_LSD2015
tokens_compound(tokens(on18$fin.y.LSD4))
tokens_compound(tokens(on18$fin.y.LSD4), data_dictionary_LSD2015)
tokens_compound(tokens(on18$fin.y.LSD4),
data_dictionary_LSD2015) %>%
dfm()
tokens_compound(tokens(on18$fin.y.LSD4),
data_dictionary_LSD2015) %>%
dfm() %>%
dfm_lookup(., data=data_dictionary_LSD2015)
tokens_compound(tokens(on18$fin.y.LSD4),
data_dictionary_LSD2015) %>%
dfm() %>%
dfm_lookup(., data_dictionary_LSD2015)
tokens_compound(tokens(on18$fin.y.LSD4),
data_dictionary_LSD2015) %>%
dfm()
# Calculate # of words about financial situation for each respnodeont
?tokens_compound
txt <- "The United Kingdom is leaving the European Union."
toks <- tokens(txt, remove_punct = TRUE)
toks
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4)
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
ntokens()
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
ntoken()
# Calculate # of words about financial situation for each respnodeont
on18$fin.y.n<-tokens(on18$fin.y.LSD4) %>%
ntoken()
on18$imm.y.n<-tokens(on18$imm.y.LSD4) %>%
ntoken()
on18 %>%
select(ends_with(".n"))
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4)
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm()
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight()
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop")
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015)
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) %>%
mutate(fin_sentiment=positive-negative)
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) %>%
data.frame()
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) %>%
data.frame() %>%
mutate(fin_sentiment=positive-negative)
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) %>%
convert(., to="data.frame")
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) %>%
convert(., to="data.frame") %>%
mutate(fin_sentiment=positive-negative)
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->fin_dfm
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->fin_dfm
tokens(on18$imm.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->imm_dfm
imm_dfm
fin_dfm
imm_dfm
fin_dfm
on18$fin.y
on18$fin.y[1:5,]
on18$fin.y[1:5]
fin_dfm
on18$fin.y[1:5]
on18$fin.y[1:5]
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() %>%fin_dfm
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() ->fin_dfm
# Calculate # of words about financial situation for each respnodeont
tokens(on18$fin.y.LSD4) %>%
dfm() ->fin_dfm
tokens(on18$imm.y.LSD4) %>%
dfm() ->imm_dfm
topfeatures(fin_dfm)
tokens_lookup(fin_dfm, dictionary=data_dictionary_LSD2015)
dfm_lookup(fin_dfm, dictionary=data_dictionary_LSD2015)
?kwic
kwic(fin_dfm, pattern=data_dictionary_LSD2015)
kwic(tokens(on18$fin.y.LSD4), pattern=data_dictionary_LSD2015)
kwic(tokens(on18$fin.y.LSD4), pattern=data_dictionary_LSD2015) %>%
View()
nrow(on18)
tokens(on18$imm.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->imm_dfm
imm_dfm
nrow(on18)
kwic(tokens(on18$fin.y.LSD4), pattern=data_dictionary_LSD2015) %>%
View()
#Get proportion counts
tokens (on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->fin_dfm
fin_dfm
#Get proportion counts
tokens (on18$fin.y.LSD4) %>%
dfm() %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->fin_dfm
fin_dfm
tokens (on18$fin.y.LSD4)
tokens_lookup(tokens(on18$fin.y.LSD4))
tokens_lookup(tokens(on18$fin.y.LSD4), dictionary=data_dictionary_LSD2015)
tokens_lookup(tokens(on18$fin.y.LSD4),
dictionary=data_dictionary_LSD2015)
# Note row 4 is showing two words that are returning negative and negative positive
# Row 5 is showing two words that are showing positive and positive.
# What are those words
kwic(tokens(on18$fin.y.LSD4), pattern=data_dictionary_LSD2015) %>%
View()
# Note row 4 is showing two words that are returning negative and negative positive
# Row 5 is showing two words that are showing positive and positive.
# What are those words
kwic(tokens(on18$fin.y.LSD4), pattern=data_dictionary_LSD2015)[4,]
# Note row 4 is showing two words that are returning negative and negative positive
# Row 5 is showing two words that are showing positive and positive.
# What are those words
kwic(tokens(on18$fin.y.LSD4), pattern=data_dictionary_LSD2015)[4:5,]
# Note row 4 is showing two words that are returning negative and negative positive
# Row 5 is showing two words that are showing positive and positive.
# What are those words
kwic(tokens(on18$fin.y.LSD4), pattern=data_dictionary_LSD2015)
#Get proportion counts
tokens (on18$fin.y.LSD4) %>%
dfm() %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->fin_dfm
fin_dfm
#Why is Item 4 showing now only one negative
colSums(fin_dfm)
#Get proportion counts
tokens (on18$fin.y.LSD4) %>%
dfm() %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->fin_dfm
tokens (on18$imm.y.LSD4) %>%
dfm() %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->imm_dfm
tokens(on18$fin.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->fin_dfm
tokens(on18$imm.y.LSD4) %>%
dfm() %>%
dfm_weight(., scheme="prop") %>%
dfm_lookup(., dictionary=data_dictionary_LSD2015) ->imm_dfm
fin_dfm
fin_dfm %>%
convert(., to="data.frame")
fin_dfm %>%
convert(., to="data.frame") %>%
mutate(fin_sentiment=positive-negative)
fin_dfm %>%
convert(., to="data.frame") %>%
mutate(fin_sentiment=positive-negative) %>%
select(fin_sentiment)
fin_dfm %>%
convert(., to="data.frame") %>%
mutate(fin_sentiment=positive-negative) %>%
select(fin_sentiment) %>%
bind_cols(on18, .)->on18
#Repeat with immigration sentiment
imm_dfm %>%
convert(., to="data.frame") %>%
mutate(imm_sentiment=positive-negative) %>%
select(imm_sentiment) %>%
bind_cols(on18, .)->on18
cor(on18$fin_sentiment, on18$imm_sentiment)
names(on18)
on18$Social_Use2
on18$Social_Use
table(on18$Social_Use, on18$Social_Use2)
table(on18$Social_Use2)
table(on18$Social_Use)
on18 %>%
select(Social_Use2)
on18 %>%
select(Social_Use2, ends_with("sentiment"))
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1)
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1) %>%
group_by(Social_Use2, name)
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1) %>%
group_by(Social_Use2, name) %>%
summarize(Average=mean(value, na.rm=T))
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1) %>%
group_by(Social_Use2, name) %>%
summarize(Average=mean(value, na.rm=T)) %>%
pivot_wider(., names_from = name, values_from=Average)
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1) %>%
group_by(Social_Use2, name) %>%
summarize(Average=mean(value, na.rm=T)) %>%
ggplot(., aes(x=Social_Use2, y=Average))+geom_point()+facet_wrap(name)
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1) %>%
group_by(Social_Use2, name) %>%
summarize(Average=mean(value, na.rm=T)) %>%
ggplot(., aes(x=Social_Use2, y=Average))+geom_point()+facet_wrap(~name)
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1) %>%
group_by(Social_Use2, name) %>%
summarize(Average=mean(value, na.rm=T)) %>%
filter(!is.na(Social_Use2)) %>%
ggplot(., aes(x=Social_Use2, y=Average))+geom_point()+facet_wrap(~name)
lm(fin_sentiment~Social_Use2, data=on18)
lm(imm_sentiment~Social_Use2, data=on18)
summary(lm(imm_sentiment~Social_Use2, data=on18))
names(on18)
on18$Media
lm(fin_sentiment~Media, data=on18)
summary(lm(fin_sentiment~Media, data=on18))
summary(lm(imm_sentiment~Media, data=on18))
source("~/OneDrive - Wilfrid Laurier University/projects_folder/ON18/Code/3_polarization.R", echo=TRUE)
warnings()
#### LOAD SCRIPTS ####
source("Code/1_load_on18.R") #clean and load dataset
source("Code/0_functions.R") #load custom functions and packages needed for these analyses
source("Code/2_LSD_emotions.R")
names(on18)
on18$fin_sentiment
on18$fin.y
#Correlate
cor(on18$fin_sentiment, on18$imm_sentiment)
on18 %>%
select(Social_Use2, ends_with("sentiment")) %>%
pivot_longer(-1) %>%
group_by(Social_Use2, name) %>%
summarize(Average=mean(value, na.rm=T)) %>%
filter(!is.na(Social_Use2)) %>%
ggplot(., aes(x=Social_Use2, y=Average))+geom_point()+facet_wrap(~name)
lm(fin_sentiment~Social_Use2, data=on18)
summary(lm(imm_sentiment~Social_Use2, data=on18))
#### LOAD SCRIPTS ####
source("Code/1_load_on18.R") #clean and load dataset
source("Code/0_functions.R") #load custom functions and packages needed for these analyses
source("Code/2_LSD_emotions.R")
<<<<<<< HEAD
>>>>>>> 85b0a28075816087ea0a3851127dc2354717b2e9
=======
#### LOAD SCRIPTS ####
source("Code/1_load_on18.R") #clean and load dataset
source("Code/0_functions.R") #load custom functions and packages needed for these analyses
source("Code/2_LSD_emotions.R")
source("Code/1_LSDprep_dec2017.R")
source("Code/2_add_emotion_variables.R")
source("Code/2_LSD_emotions.R")
source("Code/2_add_emotion_variables.R")
#### Export to SPSS File ####
names(on18)
#| label: setup
#| echo: false
library(knitr)
opts_knit$set(root.dir=rprojroot::find_rstudio_root_file())
#opts_chunk$set(warning=F,message=F)
#| label: import
#| echo: false
#| warning: false
#| message: false
#| results: 'hide'
#| output: false
source("Code/3_polarization.R", local = knitr::knit_global())
#### LOAD SCRIPTS ####
source("Code/1_load_on18.R") #clean and load dataset
source("Code/0_functions.R") #load custom functions and packages needed for these analyses
source("Code/2_LSD_emotions.R")
#| label: import
#| echo: false
#| warning: false
#| message: false
#| results: 'hide'
#| output: false
source("Code/3_polarization.R", local = knitr::knit_global())
#load libraries
library(tidyverse)
library(haven)
library(here)
library(labelled)
library(broom)
library(ggeffects)
library(modelsummary)
library(ggpubr)
#### Import Data####
on18<-read_sav(file=here("Data/Ontario ES 2018 LISPOP.sav"))
lookfor(dat, "Date")
lookfor(dat, "date")
lookfor("date", dat)
lookfor(on18, "date")
paste(format(min(on18$datestamp),"%B %d), format(max(on18$datestamp), "%d, %Y)) sep="-")
paste(format(min(on18$datestamp),"%B %d), format(max(on18$datestamp), "%d, %Y)), sep="-")
paste(format(min(on18$datestamp),"%B %d), format(max(on18$datestamp), "%d, %Y), sep="-")
nrow(on18)
paste(format(min(on18$datestamp),"%B %d"))
paste(
format(min(on18$datestamp),"%B %d"),
format(min(on18$datestamp),"%B %d"),
sep="-"
)
paste(
format(min(on18$datestamp),"%B %d"),
format(min(on18$datestamp),"%d"),
sep="-"
)
paste(
format(min(on18$datestamp),"%B %d"),
format(max(on18$datestamp),"%d"),
sep="-"
)
paste(
format(min(on18$datestamp),"%B %d"),
format(max(on18$datestamp),"%d %Y"),
sep="-"
)
paste(
format(min(on18$datestamp),"%B %d"),
format(max(on18$datestamp),"%d %Y"),
sep=","
)
paste(
format(min(on18$datestamp),"%B %d"),
format(max(on18$datestamp),"%B %d %Y"),
sep=","
)
paste(
format(min(on18$datestamp),"%B %d"),
format(max(on18$datestamp),"%B %d %Y"),
sep="-"
)
paste(
format(min(on18$datestamp),"%B %d"),
format(max(on18$datestamp),"%B %d, %Y"),
sep="-"
)
>>>>>>> Simon_writing
